{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a36d3b1-1a8d-478e-8060-6477472e6bd4",
   "metadata": {},
   "source": [
    "# **Problem Statement**  \n",
    "## **5. Implement a simple Naive Bayes classifier using probability rules.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8eaf4-c67c-4ef9-87dd-a1c242b88a5a",
   "metadata": {},
   "source": [
    "Implement a Naive Bayes classifier from scratch using basic probability rules.\n",
    "\n",
    "Given a labeled dataset:\n",
    "- Compute prior probabilities for each class\n",
    "- Compute likelihood probabilities for feature values\n",
    "- Apply Bayes’ Theorem to predict the class of unseen data points\n",
    "No external ML libraries (like sklearn) are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b96244-784f-44f1-b72a-e69e054e8b67",
   "metadata": {},
   "source": [
    "### Constraints & Example Inputs/Outputs\n",
    "\n",
    "- Classification problem (binary or multi-class)\n",
    "- Categorical features (simplest Naive Bayes)\n",
    "- Conditional independence assumption holds\n",
    "- No missing values\n",
    "\n",
    "Example Input:\n",
    "```python\n",
    "X = [\n",
    "    [\"Sunny\", \"Hot\", \"High\", \"Weak\"],\n",
    "    [\"Sunny\", \"Hot\", \"High\", \"Strong\"],\n",
    "    [\"Overcast\", \"Hot\", \"High\", \"Weak\"],\n",
    "    [\"Rain\", \"Mild\", \"High\", \"Weak\"]\n",
    "]\n",
    "\n",
    "y = [\"No\", \"No\", \"Yes\", \"Yes\"]\n",
    "\n",
    "```\n",
    "Example Prediction Input\n",
    "```python\n",
    "[\"Sunny\", \"Mild\", \"High\", \"Weak\"]\n",
    "```\n",
    "\n",
    "Expected Output:\n",
    "```python\n",
    "Predicted Class → No\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e7cad-ea42-4bb3-a20e-a91cba6d7424",
   "metadata": {},
   "source": [
    "### Solution Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b599f-addb-4686-8425-1f6e6f862acd",
   "metadata": {},
   "source": [
    "### Naive Bayes Logic\n",
    "\n",
    "1. Bayes’ Theorem:\n",
    "\n",
    "   ```python\n",
    "      P(Class∣X)= P(X∣Class)⋅P(Class) / P(X)\n",
    "    ```\n",
    "    \n",
    "2. Since P(X) is constant for all classes:\n",
    "\n",
    "   ```python\n",
    "   P(Class∣X)∝P(Class)⋅∏P(xi​∣Class)\n",
    "   ```\n",
    "\n",
    "\n",
    "Steps\n",
    "\n",
    "1. Compute prior probabilities:\n",
    "\n",
    "P(Class)\n",
    "\n",
    "2. Compute likelihood probabilities:\n",
    "\n",
    "P(feature value∣Class)\n",
    "\n",
    "3. Multiply probabilities for prediction\n",
    "\n",
    "4. Choose class with maximum posterior probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be931aed-e6ac-4398-a9cc-cd43c98ed91b",
   "metadata": {},
   "source": [
    "### Solution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cab10a-c58c-459b-b504-16dc7ee1d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach1: Brute Force Naive Bayes (Direct Probability Computation)\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NaiveBayesBruteForce:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods = {}\n",
    "        self.classes = set()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = set(y)\n",
    "        total_samples = len(y)\n",
    "        class_counts = Counter(y)\n",
    "\n",
    "        # Prior probabilities\n",
    "        for cls in self.classes:\n",
    "            self.class_priors[cls] = class_counts[cls] / total_samples\n",
    "\n",
    "        # Likelihoods\n",
    "        self.likelihoods = {cls: defaultdict(lambda: defaultdict(int)) for cls in self.classes}\n",
    "\n",
    "        for features, label in zip(X, y):\n",
    "            for idx, value in enumerate(features):\n",
    "                self.likelihoods[label][idx][value] += 1\n",
    "\n",
    "        # Convert counts to probabilities\n",
    "        for cls in self.classes:\n",
    "            for idx in self.likelihoods[cls]:\n",
    "                total = sum(self.likelihoods[cls][idx].values())\n",
    "                for val in self.likelihoods[cls][idx]:\n",
    "                    self.likelihoods[cls][idx][val] /= total\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for features in X:\n",
    "            class_scores = {}\n",
    "\n",
    "            for cls in self.classes:\n",
    "                score = self.class_priors[cls]\n",
    "                for idx, value in enumerate(features):\n",
    "                    score *= self.likelihoods[cls][idx].get(value, 1e-6)\n",
    "                class_scores[cls] = score\n",
    "\n",
    "            predictions.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febed86-bc77-49f3-aa50-7adfe033688a",
   "metadata": {},
   "source": [
    "### Alternative Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0f40fa-e3cf-49eb-b872-7290e4cc7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach2: Optimized Naive Bayes (Log Probabilities – Numerically Stable)\n",
    "import math\n",
    "\n",
    "class NaiveBayesOptimized:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods = {}\n",
    "        self.classes = set()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = set(y)\n",
    "        total_samples = len(y)\n",
    "        class_counts = Counter(y)\n",
    "\n",
    "        # Log Priors\n",
    "        for cls in self.classes:\n",
    "            self.class_priors[cls] = math.log(class_counts[cls] / total_samples)\n",
    "\n",
    "        self.likelihoods = {cls: defaultdict(lambda: defaultdict(int)) for cls in self.classes}\n",
    "\n",
    "        for features, label in zip(X, y):\n",
    "            for idx, value in enumerate(features):\n",
    "                self.likelihoods[label][idx][value] += 1\n",
    "\n",
    "        # Log Likelihoods\n",
    "        for cls in self.classes:\n",
    "            for idx in self.likelihoods[cls]:\n",
    "                total = sum(self.likelihoods[cls][idx].values())\n",
    "                for val in self.likelihoods[cls][idx]:\n",
    "                    self.likelihoods[cls][idx][val] = math.log(\n",
    "                        self.likelihoods[cls][idx][val] / total\n",
    "                    )\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for features in X:\n",
    "            scores = {}\n",
    "\n",
    "            for cls in self.classes:\n",
    "                score = self.class_priors[cls]\n",
    "                for idx, value in enumerate(features):\n",
    "                    score += self.likelihoods[cls][idx].get(value, math.log(1e-6))\n",
    "                scores[cls] = score\n",
    "\n",
    "            predictions.append(max(scores, key=scores.get))\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164d9bc-0e3b-4391-892e-51cfd51a887a",
   "metadata": {},
   "source": [
    "### Alternative Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf3ad0-4f06-4697-87d4-8ed734705304",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes (for numerical features)\n",
    "- Multinomial Naive Bayes (for text data)\n",
    "- Bernoulli Naive Bayes (binary features)\n",
    "- Laplace smoothing for zero-frequency issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31204571-03a3-42ae-8461-d52da4c9a353",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1914f61b-475e-4c5c-937e-9fa0304509ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['No']\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Weather Dataset (Classic Example)\n",
    "\n",
    "X = [\n",
    "    [\"Sunny\", \"Hot\", \"High\", \"Weak\"],\n",
    "    [\"Sunny\", \"Hot\", \"High\", \"Strong\"],\n",
    "    [\"Overcast\", \"Hot\", \"High\", \"Weak\"],\n",
    "    [\"Rain\", \"Mild\", \"High\", \"Weak\"],\n",
    "    [\"Rain\", \"Cool\", \"Normal\", \"Weak\"],\n",
    "    [\"Rain\", \"Cool\", \"Normal\", \"Strong\"],\n",
    "    [\"Overcast\", \"Cool\", \"Normal\", \"Strong\"]\n",
    "]\n",
    "\n",
    "y = [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\"]\n",
    "\n",
    "\n",
    "model = NaiveBayesOptimized()\n",
    "model.fit(X, y)\n",
    "\n",
    "test_sample = [[\"Sunny\", \"Mild\", \"High\", \"Weak\"]]\n",
    "print(\"Prediction:\", model.predict(test_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e708bdb5-7e54-4716-82a9-837bdb954272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Multiple Predictions\n",
    "\n",
    "X_test = [\n",
    "    [\"Rain\", \"Cool\", \"Normal\", \"Weak\"],\n",
    "    [\"Overcast\", \"Hot\", \"High\", \"Strong\"]\n",
    "]\n",
    "\n",
    "print(model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867862d0-aa62-4ee8-95aa-72e6d6520c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True', 'False']\n"
     ]
    }
   ],
   "source": [
    "# Test Case 3: Binary Feature Dataset\n",
    "\n",
    "X = [\n",
    "    [\"Yes\", \"Yes\"],\n",
    "    [\"Yes\", \"No\"],\n",
    "    [\"No\", \"Yes\"],\n",
    "    [\"No\", \"No\"]\n",
    "]\n",
    "\n",
    "y = [\"True\", \"True\", \"False\", \"False\"]\n",
    "\n",
    "model = NaiveBayesBruteForce()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict([[\"Yes\", \"Yes\"], [\"No\", \"Yes\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462318bf-672f-4468-9122-070cf70b5af5",
   "metadata": {},
   "source": [
    "## Complexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff61a2e-affa-44cc-b483-573f9dc9b166",
   "metadata": {},
   "source": [
    "### Time Complexity\n",
    "- Training: O(n × d)\n",
    "- Prediction: O(c × d) per sample\n",
    "\n",
    "### Space Complexity\n",
    "- O(c × d × v)\n",
    "where v = unique feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae9b22-a3fd-4d97-807c-9deb388d03b1",
   "metadata": {},
   "source": [
    "#### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
